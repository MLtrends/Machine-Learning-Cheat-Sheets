{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScikitLearnCheatSheet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObmce/Tx2WTbo06KExpsL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bomlme/Machine-Learning-Cheat-Sheets/blob/main/Scikit-Learn%20Cheat%20Sheet/ScikitLearnCheatSheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Scikit-learn"
      ],
      "metadata": {
        "id": "UOM0dCA-nUux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workflow of a basic example"
      ],
      "metadata": {
        "id": "L85O_wL_mNUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris() #1.Preprocessing\n",
        "X, y = iris.data[:,:3], iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "X_train_std = StandardScaler().fit_transform(X_train)\n",
        "X_test_std = StandardScaler().fit_transform(X_test)\n",
        "knc = KNeighborsClassifier(n_neighbors=3) #2.Model creation\n",
        "knc.fit(X_train_std,y_train) #3.Model fitting\n",
        "y_pred = knc.predict(X_test_std) #4.Prediction\n",
        "accuracy_score(y_test, y_pred) #5.Performance evaluation"
      ],
      "metadata": {
        "id": "p09DKMHTmMsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Preprocessing\n",
        "https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing"
      ],
      "metadata": {
        "id": "nduhuCm8IMju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Number of Instances:\n",
        "150 (50 in each of three classes)\n",
        "* Attributes:\n",
        "sepal length, sepal width, petal length, petal width (cm)\n",
        "* Class:\n",
        "Iris-Setosa, Iris-Versicolour, Iris-Virginica"
      ],
      "metadata": {
        "id": "Ij4I8nWfS2Py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uccHu_HHnQYq"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data[:,:4], iris.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
      ],
      "metadata": {
        "id": "_21BO-DJo7xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When features of input dataset have large differences between their ranges\n",
        "* Want to ensure zero mean and unit standard deviation\n",
        "* Each of the attributes contributes equally to the analysis\n",
        "* Standardization does not have a bounding range, can remove outliers\n",
        "* X_standardized = StandardScaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "PqqpFlSruQm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization (z score = (x – μ) / σ ) \n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_standardized = scaler.transform(X_train)\n",
        "X_test_standardized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "_N35ohZ0p9-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transform features to be on a similar scale, e.g. [0 1],[-1 1]\n",
        "* When features are of different scales.\n",
        "* When distribution of data does not follow a Gaussian distribution\n",
        "* X_normalized = Normalizer.fit_transform(X_train)\n"
      ],
      "metadata": {
        "id": "TSuTPYBzugA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization ((X - X_min)/(X_max - X_min)) \n",
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer().fit(X_train)\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "5FftlmdYCcv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* X_binarized = binarizer.fit_transform(X_train)\n"
      ],
      "metadata": {
        "id": "DJChwnBzyT6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarization (numerical features to boolean values)\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold = 3).fit(X_train)\n",
        "X_binarized = binarizer.transform(X_test)"
      ],
      "metadata": {
        "id": "OH63JGDLDhtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputation of Missing Values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "oFN-iucnFkuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Generate a new feature matrix consisting of all polynomial combinations \n",
        "* of the features with degree less than or equal to the specified degree\n",
        "* Add new interaction features this way"
      ],
      "metadata": {
        "id": "wINWYH0Xuzxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating polynomial features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(2)\n",
        "X_poly = poly.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "SIyTEK77Mu6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Log Transform helps to handle skewed data\n",
        "* The distribution becomes more approximate to normal after it\n",
        "* Log transform normalizes the magnitude differences\n",
        "* It also decreases the effect of the outliers\n",
        "* Forwards the arguments to a user-defined function"
      ],
      "metadata": {
        "id": "RS-EvfT1u5jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformers\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "transformer = FunctionTransformer(np.log1p, validate=True)\n",
        "X_transformed = transformer.transform(X_train)"
      ],
      "metadata": {
        "id": "7m28sNozNcNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Make the model more robust and prevent overfitting\n",
        "* It has a cost to the performance.\n",
        "* Binning can be applied on both categorical and numerical data\n",
        "* Bin continuous data (features in columns) into intervals\n"
      ],
      "metadata": {
        "id": "PxbbGBdTvA9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "k_bins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans').fit(X)\n",
        "est = k_bins.transform(X_train)"
      ],
      "metadata": {
        "id": "NxXHI_QtOMPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Encoding Categorical Features \n",
        "* Use this when attributes are nominal (mutually exclusive)\n",
        "* it can take a multidimensional array"
      ],
      "metadata": {
        "id": "DOXI9YjQvx7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder() \n",
        "enc.fit(y.reshape(-1,1))  \n",
        "enc.transform(y.reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "eiMQYEQlEQl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use this when attributes are ordinal\n"
      ],
      "metadata": {
        "id": "ghIY5ciWv1O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoder \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "enc = LabelEncoder()\n",
        "y_encoded = enc.fit_transform(y)"
      ],
      "metadata": {
        "id": "7Nxb9a1mJNSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2). Model Creation"
      ],
      "metadata": {
        "id": "rMBtsU-IOgG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised learning"
      ],
      "metadata": {
        "id": "JykTUxfdkPUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes.\n",
        "<img src=\"https://66.media.tumblr.com/ff709fe1c77091952fb3e3e6af91e302/tumblr_inline_o9aa8dYRkB1u37g00_540.png\"><br>\n",
        "<img src=\"https://66.media.tumblr.com/7f12391977435370c1ddf4945dca0575/tumblr_inline_o9aa9nH3WQ1u37g00_540.png\" width=\"300\"><br>\n",
        "The idea is that the data will continue to be mapped into higher and higher dimensions until a hyperplane can be formed to segregate it.<br>\n",
        "<img src=\"https://66.media.tumblr.com/9bffea56372d28d2a30f80557451e824/tumblr_inline_o9aabehtqP1u37g00_540.png\">"
      ],
      "metadata": {
        "id": "P1m4N_dpjRfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
      ],
      "metadata": {
        "id": "ZrYfLFcJmnCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supervised Learning Estimators - classification\n",
        "\n",
        "# Support Vector Machines (SVM)\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# K Nearest Neighbor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knc = KNeighborsClassifier(n_neighbors=3)"
      ],
      "metadata": {
        "id": "B7Ze1meXOqtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supervised Learning Estimators - regression\n",
        "\n",
        "# Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "\n",
        "# K Nearest Neighbor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knr = KNeighborsRegressor(n_neighbors=2)"
      ],
      "metadata": {
        "id": "sJ3ZutKnO1n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsupervised learning"
      ],
      "metadata": {
        "id": "YWInYdxXkZ41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsuperviser Learning Estimators\n",
        "\n",
        "# K means\n",
        "from sklearn.cluster import KMeans\n",
        "k_means = KMeans(n_clusters = 3, random_state= 0)\n",
        "\n",
        "# PCA\n",
        "# Reduce number of attributes, while presrving as much info as possible\n",
        "# Use Singular Value Decomposition of the data to project it to a lower dimensional space\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_train)"
      ],
      "metadata": {
        "id": "2Wp0iTlXO393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ceb1bad-e3b0-4e9f-b8ef-effd301ec1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model fitting\n",
        "# Supervised learning\n",
        "clf = svc.fit(X_train, y_train)\n",
        "clf = knc.fit(X_train, y_train)\n",
        "clf = gnb.fit(X_train, y_train)\n",
        "\n",
        "# Unsupervised learning\n",
        "reg = k_means.fit(X_train)\n",
        "pca_model = pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "6Yi8n6WMWPSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "# Supervised learning\n",
        "y_pred = svc.predict(X_test)\n",
        "y_pred = lr.predict(X_test)\n",
        "y_pred = knc.predict(X_test)\n",
        "\n",
        "# Unsupervised learning\n",
        "y_pred = k_means.predict(X_test)"
      ],
      "metadata": {
        "id": "OA0R2WHmW62p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3). Model Performance Evaluation"
      ],
      "metadata": {
        "id": "KrCWNH6RXhnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifcation Metrics\n",
        "\n",
        "# Accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "knc.score(X_test,y_test)\n",
        "accuracy_score(y_test,y_pred)\n",
        "\n",
        "# Classfication Report\n",
        "from sklearn.metrics import classification_report\n",
        "classification_report(y_test,y_pred)\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Ff0lSxw3XlDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Metrics\n",
        "\n",
        "# Mean Absolute Error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Mean Squared Error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# R^2 score\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "CSv3CpmmYfaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering Metrics\n",
        "\n",
        "# Adjusted Rand Index\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "adjusted_rand_score(y, y_pred)\n",
        "\n",
        "# Homogeneity\n",
        "from sklearn.metrics import homogeneity_score\n",
        "homogeneity_score(y_pred, y_pred)\n",
        "\n",
        "# V-measure\n",
        "from sklearn.metrics import v_measure_score\n",
        "v_measure_score(y, y_pred)"
      ],
      "metadata": {
        "id": "zXFehL6KZdcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" width = \"300\"><br>\n",
        "When evaluating different settings (“hyperparameters”) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.<br>\n",
        "\n",
        "Training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.<br>\n",
        "\n",
        "By partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.<br>\n",
        "\n",
        "* k-fold cross-validation\n",
        "1. A model is trained using k-1  of the folds as training data;\n",
        "2. The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"500\">"
      ],
      "metadata": {
        "id": "s_gFNPukSVp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-validation\n",
        "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "clf = svc.fit(X_train, y_train)\n",
        "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')"
      ],
      "metadata": {
        "id": "jo0FcybIaBl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4). Model Tuning"
      ],
      "metadata": {
        "id": "AqGm0-ubafRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(X_train, y_train) \n",
        "# To check the results\n",
        "clf.cv_results_"
      ],
      "metadata": {
        "id": "2UKIpMlzalVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomized Parameter Optimization\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "params ={'n_neighbors': [2,3,4], 'weights':['uniform','distance']}\n",
        "clf = RandomizedSearchCV(estimator=knc,\n",
        "                             param_distributions=params,\n",
        "                             cv=4,\n",
        "                             n_iter=8,\n",
        "                             random_state=5)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.cv_results_"
      ],
      "metadata": {
        "id": "k5XELYXibb8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Guy(object):\n",
        "\n",
        "    # attributes\n",
        "    name = \"Bo\"\n",
        "    work = \"Cal Poly\"\n",
        "    home = \"California\"\n",
        "    hobby = \"Traveling\"\n",
        "    interests = \"Machine Learning, Mechatronics\"\n",
        "    \n",
        "    # methods\n",
        "    def update_status(self, string):\n",
        "        print(\"i'm working on\", string, \"now!\")\n",
        "        \n",
        "    def introduce(self):\n",
        "        print(\"Hello, this is\", self.name)\n",
        "        print(\"I'm into\", self.interests, \"and\", self.hobby)\n",
        "\n",
        "Bo = Guy()\n",
        "Bo.introduce()\n",
        "Bo.update_status(\"deep learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeseQlpL3l4A",
        "outputId": "b7f45034-dd9e-4f11-8193-8235eceb0a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, this is Bo\n",
            "I'm into Machine Learning, Mechatronics and Traveling\n",
            "i'm working on deep learning now!\n"
          ]
        }
      ]
    }
  ]
}