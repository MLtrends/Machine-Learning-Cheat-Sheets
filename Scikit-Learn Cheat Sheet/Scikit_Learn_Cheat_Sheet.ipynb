{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scikit Learn Cheat Sheet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPX4izGC5zpZ/jf16g4v3uQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bomlme/Machine-Learning-Cheat-Sheets/blob/main/Scikit-Learn%20Cheat%20Sheet/Scikit_Learn_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit-Learn Cheat Sheet\n",
        "Scikit-learn is a robust and popular library for machine learning based on Python, NumPy, SciPy and Matplotlib. It consists a selection of common tools for machine learning and statistical modeling, for example, classification, regression, clustering and dimensionality reduction. This document summaries the functions you may use with Scikit-Learn."
      ],
      "metadata": {
        "id": "UOM0dCA-nUux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow of a basic example\n",
        "This section shows the workflow of solving a problem using machine learning."
      ],
      "metadata": {
        "id": "L85O_wL_mNUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris() #1.Preprocessing\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "X_train_std = StandardScaler().fit_transform(X_train)\n",
        "X_test_std = StandardScaler().fit_transform(X_test)\n",
        "knc = KNeighborsClassifier(n_neighbors=3) #2.Model creation\n",
        "knc.fit(X_train_std,y_train) #3.Model fitting\n",
        "y_pred = knc.predict(X_test_std) #4.Prediction\n",
        "accuracy_score(y_test, y_pred) #5.Performance evaluation"
      ],
      "metadata": {
        "id": "p09DKMHTmMsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Preprocessing\n"
      ],
      "metadata": {
        "id": "nduhuCm8IMju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data set\n",
        "The data set here consists of a 150x4 numpy.ndarray of 3 different types of irises (Setosa, Versicolour, and Virginica). The rows are the samples and the columns areg Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
        "* Number of Instances:\n",
        "150 (50 in each of three classes)\n",
        "* Attributes:\n",
        "sepal length, sepal width, petal length, petal width (cm)\n",
        "* Class:\n",
        "Iris-Setosa, Iris-Versicolour, Iris-Virginica"
      ],
      "metadata": {
        "id": "Ij4I8nWfS2Py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uccHu_HHnQYq"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data[:,:4], iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Taining and test data\n",
        "train_test_split: this function split arrays or matrices into random train and test subsets. The default proprtions are 0.75 and 0.25"
      ],
      "metadata": {
        "id": "PpkTi5tHo_nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
      ],
      "metadata": {
        "id": "_21BO-DJo7xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling - Standardization\n",
        "Standardization of datasets is a common requirement for many machine learning estimators, and it is needed when:\n",
        "* Distribution of the data is normal distribution\n",
        "* Features of input dataset have large differences between their ranges\n",
        "* Zero mean and unit standard deviation are needed\n",
        "* Each of the attributes need to contribute equally to the analysis\n",
        "* Remove outliers impact (standardization does not have a bounding range)"
      ],
      "metadata": {
        "id": "PqqpFlSruQm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization (z score = (x – μ) / σ ) \n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_standardized = scaler.transform(X_train)\n",
        "X_test_standardized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "_N35ohZ0p9-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling - Normalization \n",
        "Normalization is the process of scaling individual samples to have unit norm, and it is useful when:\n",
        "* Distribution of data is unknown.\n",
        "* Quadratic form (dot-product or any other kernel) is used to quantify the similarity of any pair of samples\n",
        "* Features need to be transformed to a similar scale, e.g. [0 1]"
      ],
      "metadata": {
        "id": "TSuTPYBzugA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization ((X - X_min)/(X_max - X_min)) \n",
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer().fit(X_train)\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "5FftlmdYCcv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binarization \n",
        "Binarization is a common operation on text count (presence or absence of a feature), and for estimators that consider boolean random variables (modelled using the Bernoulli distribution in a Bayesian setting)."
      ],
      "metadata": {
        "id": "GH4FMgP_6cat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarization (numerical features to boolean values)\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold = 3).fit(X_train)\n",
        "X_binarized = binarizer.transform(X_test)"
      ],
      "metadata": {
        "id": "OH63JGDLDhtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation\n",
        "It is used to impute the missing values and infer them from the known part of the data.\n",
        "* Univariate imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. impute.SimpleImputer). \n",
        "* Multivariate use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer)."
      ],
      "metadata": {
        "id": "BHGAEAhW7BWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputation of Missing Values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "oFN-iucnFkuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Feature Transforms\n",
        "Input features may interact in many ways, engineering new features can expose these interactions and see potential improved model performance.\n",
        "* Generate a new feature matrix consisting of all specified polynomial combinations of the features degree\n",
        "* Add new interaction features"
      ],
      "metadata": {
        "id": "wINWYH0Xuzxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating polynomial features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(2)\n",
        "X_poly = poly.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "SIyTEK77Mu6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Transformers\n",
        "It is used to convert an existing Python function into a transformer to assist in data cleaning or processing. The the popular log transform returns the natural logarithm of one plus the input array, element-wise.\n",
        "* Log Transform helps to handle skewed data\n",
        "* The distribution becomes more approximate to normal after the transform\n",
        "* Log transform normalizes the magnitude differences\n",
        "* It also decreases the effect of the outliers\n",
        "* Forwards the arguments to a user-defined function"
      ],
      "metadata": {
        "id": "RS-EvfT1u5jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformers\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "transformer = FunctionTransformer(np.log1p, validate=True)\n",
        "X_transformed = transformer.transform(X_train)"
      ],
      "metadata": {
        "id": "7m28sNozNcNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binning \n",
        "Binning is applied to data\n",
        "* Make the model more robust and prevent overfitting\n",
        "* Binning can be applied on both categorical and numerical data\n",
        "* Bin continuous data (features in columns) into intervals\n",
        "* It has a cost to the performance though\n",
        "\n"
      ],
      "metadata": {
        "id": "PxbbGBdTvA9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "k_bins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans').fit(X)\n",
        "est = k_bins.transform(X_train)"
      ],
      "metadata": {
        "id": "NxXHI_QtOMPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Encoding\n",
        "One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. One-hot encoder converts each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector.\n",
        "* Encoding Categorical Features \n",
        "* Use this when attributes are nominal (mutually exclusive)\n",
        "* it can take a multidimensional array"
      ],
      "metadata": {
        "id": "DOXI9YjQvx7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder() \n",
        "enc.fit(y.reshape(-1,1))  \n",
        "enc.transform(y.reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "eiMQYEQlEQl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding\n",
        "Label Encoding converts labels into a numeric form. Machine learning algorithms can then decide in a better way how those labels must be operated.\n",
        "* Use this when attributes are ordinal\n"
      ],
      "metadata": {
        "id": "ghIY5ciWv1O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoder \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "enc = LabelEncoder()\n",
        "y_encoded = enc.fit_transform(y)"
      ],
      "metadata": {
        "id": "7Nxb9a1mJNSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2). Model Creation"
      ],
      "metadata": {
        "id": "rMBtsU-IOgG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised learning - classfication\n",
        "Supervised learning is an approach to creating AI algorithms to be trained on input data and \"correct\" output. The model is trained until it can detect the underlying patterns and relationships between the input data and the output labels. This enables the model to predict results given new input data. classification is a supervised learning concept which basically categorizes a set of data into classes."
      ],
      "metadata": {
        "id": "JykTUxfdkPUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supervised Learning Estimators - classification\n",
        "\n",
        "# Support Vector Machines (SVM)\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# K Nearest Neighbor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knc = KNeighborsClassifier(n_neighbors=3)"
      ],
      "metadata": {
        "id": "B7Ze1meXOqtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Support Vector Machines (SVM)\n",
        "SVMs are based on the idea of finding a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points. The idea is that the data will continue to be mapped into higher and higher dimensions until a hyperplane can be formed to segregate it.<br>\n",
        "<img src=\"https://github.com/bomlme/Machine-Learning-Cheat-Sheets/blob/main/assets/svm1.png?raw=true\" width=\"400\"><br>\n",
        "SVM works without any modifications for linearly separable data.Kernelized SVM can be used for non-linearly separable data.<br>\n",
        "<img src=\"https://github.com/bomlme/Machine-Learning-Cheat-Sheets/blob/main/assets/svm2.png?raw=true\" width=\"400\">"
      ],
      "metadata": {
        "id": "P1m4N_dpjRfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Naive Bayes\n",
        "Bayes' Theorem is useful when working with conditional probabilities, and it provides us with a way to reverse them:<br>\n",
        "$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$<br>\n",
        "For example, if we want to the determine the possibility of the sentence \"had a great night\" is postive can be expressed like this:\n",
        "$P(positive|had\\,a\\,great\\,night) = \\frac{P(had\\,a\\,great\\,night|positive)*P(positive)}{P(had\\,a\\,great\\,night)}$<br>\n",
        "So we just need to compare:\n",
        "${P(had\\,a\\,great\\,night|positive)*P(positive)}$ with ${P(had\\,a\\,great\\,night|negative)*P(negative)}$<br>\n",
        "To simplify the calculations, we need to be Naive: we assume that every word in a sentence is independent of the other ones. <br>\n",
        "So, ${P(had\\,a\\,great\\,night)}$ becomes ${P(had)*P(a)*P(great)*P(night)}$<br>\n",
        "${P(had\\,a\\,great\\,night|positive)}$ becomes ${P(had|positive)*P(a|positive)*P(great|positive)*P(night|positive)}$<br>\n",
        "Now based on how often these individual words show up in the training data, and we can calculate them."
      ],
      "metadata": {
        "id": "zDbn_MFUoenA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Learning - regression\n"
      ],
      "metadata": {
        "id": "8dwdlccJBZMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supervised Learning Estimators - regression\n",
        "\n",
        "# Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "\n",
        "# K Nearest Neighbor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knr = KNeighborsRegressor(n_neighbors=2)"
      ],
      "metadata": {
        "id": "sJ3ZutKnO1n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Regression\n",
        "inear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). This method is mostly used for forecasting and finding out cause and effect linear relationship."
      ],
      "metadata": {
        "id": "ihcnFbSt1hgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K nearest neighbor (KNN)\n",
        "KNN is a lazy learning algorithm which stores all instances corresponding to training data in n-dimensional space.KNN does not focus on constructing a general internal model, but it works on storing instances of training data.<br>\n",
        "Classification is calculated from a simple majority vote of the k nearest neighbors of each point, so whichever label most of the neighbors have is the label for the new point.<br>\n",
        "<img src=\"https://miro.medium.com/max/1151/0*ItVKiyx2F3ZU8zV5\" width=\"400\">"
      ],
      "metadata": {
        "id": "GSCvx-15tsYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsupervised learning"
      ],
      "metadata": {
        "id": "YWInYdxXkZ41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsuperviser Learning Estimators\n",
        "\n",
        "# K means\n",
        "from sklearn.cluster import KMeans\n",
        "k_means = KMeans(n_clusters = 3, random_state= 0)\n",
        "\n",
        "# PCA\n",
        "# Reduce number of attributes, while presrving as much info as possible\n",
        "# Use Singular Value Decomposition of the data to project it to a lower dimensional space\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_train)"
      ],
      "metadata": {
        "id": "2Wp0iTlXO393"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K means\n",
        "A K-means clustering algorithm tries to group similar items in the form of clusters. The number of groups is represented by K. \n",
        "1. K points are placed into the object data space representing the initial group of centroids. \n",
        "2. Each data point is then assigned into the closest cluster through reducing the in-cluster variance. \n",
        "3. After all points are assigned, the positions of the k centroids are recalculated using the k clusters. \n",
        "4. Steps 2 and 3 are repeated until the positions of the centroids no longer move.<br>\n",
        "In summary, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the clusters as small as possible."
      ],
      "metadata": {
        "id": "1J94p53d39Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Principal component analysis (PCA)\n",
        "PCA is a way to bring out strong patterns from large and complex datasets. It finds a way to reduce the dimensions of the data by projecting it onto lines drawn through data, starting with the line that goes through the data in the direction of the greatest variance. This is calculated by looking at the eigenvectors of the covariance matrix. The eigenvector with the largest eigenvalue is the first principal component, the eigenvector with the second largest eigenvalue is the second principal component, etc."
      ],
      "metadata": {
        "id": "FJH-HtYO9xNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model fitting\n",
        "# Supervised learning\n",
        "clf = svc.fit(X_train, y_train)\n",
        "clf = knc.fit(X_train, y_train)\n",
        "clf = gnb.fit(X_train, y_train)\n",
        "# Unsupervised learning\n",
        "reg = k_means.fit(X_train)\n",
        "pca_model = pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "6Yi8n6WMWPSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "# Supervised learning\n",
        "y_pred = svc.predict(X_test)\n",
        "y_pred = lr.predict(X_test)\n",
        "y_pred = knc.predict(X_test)\n",
        "# Unsupervised learning\n",
        "y_pred = k_means.predict(X_test)"
      ],
      "metadata": {
        "id": "OA0R2WHmW62p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3). Model Performance Evaluation"
      ],
      "metadata": {
        "id": "KrCWNH6RXhnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifcation Metrics\n",
        "\n",
        "# Accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "knc.score(X_test,y_test)\n",
        "accuracy_score(y_test,y_pred)\n",
        "\n",
        "# Classfication Report\n",
        "from sklearn.metrics import classification_report\n",
        "classification_report(y_test,y_pred)\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Ff0lSxw3XlDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Metrics\n",
        "\n",
        "# Mean Absolute Error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Mean Squared Error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# R^2 score\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "CSv3CpmmYfaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering Metrics\n",
        "\n",
        "# Adjusted Rand Index\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "adjusted_rand_score(y, y_pred)\n",
        "\n",
        "# Homogeneity\n",
        "from sklearn.metrics import homogeneity_score\n",
        "homogeneity_score(y_pred, y_pred)\n",
        "\n",
        "# V-measure\n",
        "from sklearn.metrics import v_measure_score\n",
        "v_measure_score(y, y_pred)"
      ],
      "metadata": {
        "id": "zXFehL6KZdcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" width = \"300\"><br>\n",
        "When evaluating different settings (“hyperparameters”) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.<br>\n",
        "\n",
        "Training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.<br>\n",
        "\n",
        "By partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.<br>\n",
        "\n",
        "* k-fold cross-validation\n",
        "1. A model is trained using k-1  of the folds as training data;\n",
        "2. The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"500\">"
      ],
      "metadata": {
        "id": "s_gFNPukSVp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-validation\n",
        "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "clf = svc.fit(X_train, y_train)\n",
        "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')"
      ],
      "metadata": {
        "id": "jo0FcybIaBl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4). Model Tuning"
      ],
      "metadata": {
        "id": "AqGm0-ubafRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(X_train, y_train) \n",
        "# To check the results\n",
        "clf.cv_results_"
      ],
      "metadata": {
        "id": "2UKIpMlzalVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomized Parameter Optimization\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "params ={'n_neighbors': [2,3,4], 'weights':['uniform','distance']}\n",
        "clf = RandomizedSearchCV(estimator=knc,\n",
        "                             param_distributions=params,\n",
        "                             cv=4,\n",
        "                             n_iter=8,\n",
        "                             random_state=5)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.cv_results_"
      ],
      "metadata": {
        "id": "k5XELYXibb8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}