{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scikit-Learn Cheat Sheet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJCiQpFYQip2GIMizNCoya"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit-Learn Cheat Sheet\n",
        "Scikit-learn is a robust and popular library for machine learning based on Python, NumPy, SciPy and Matplotlib. It consists a selection of common tools for machine learning and statistical modeling, for example, classification, regression, clustering and dimensionality reduction. This document summaries the functions you may use with Scikit-Learn."
      ],
      "metadata": {
        "id": "UOM0dCA-nUux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow of a basic example\n",
        "This section shows the workflow of solving a classfication problem using machine learning."
      ],
      "metadata": {
        "id": "L85O_wL_mNUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris() #1.Preprocessing\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "X_train_std = StandardScaler().fit_transform(X_train)\n",
        "X_test_std = StandardScaler().fit_transform(X_test)\n",
        "knc = KNeighborsClassifier(n_neighbors=3) #2.Model creation\n",
        "knc.fit(X_train_std,y_train) #3.Model fitting\n",
        "y_pred = knc.predict(X_test_std) #4.Prediction\n",
        "accuracy_score(y_test, y_pred) #5.Performance evaluation"
      ],
      "metadata": {
        "id": "p09DKMHTmMsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Data Preprocessing\n"
      ],
      "metadata": {
        "id": "nduhuCm8IMju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data set\n",
        "The data set here consists of a 150x4 numpy.ndarray of 3 different types of irises (Setosa, Versicolour, and Virginica). The rows are the samples and the columns areg Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
        "* Number of Instances:\n",
        "150 (50 in each of three classes)\n",
        "* Attributes:\n",
        "sepal length, sepal width, petal length, petal width (cm)\n",
        "* Class:\n",
        "Iris-Setosa, Iris-Versicolour, Iris-Virginica"
      ],
      "metadata": {
        "id": "Ij4I8nWfS2Py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uccHu_HHnQYq"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data[:,:4], iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Taining and test data\n",
        "train_test_split: this function split arrays or matrices into random train and test subsets. The default proprtions are 0.75 and 0.25"
      ],
      "metadata": {
        "id": "PpkTi5tHo_nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
      ],
      "metadata": {
        "id": "_21BO-DJo7xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling - Standardization\n",
        "Standardization of datasets is a common requirement for many machine learning estimators, and it is needed when:\n",
        "* Distribution of the data is normal distribution\n",
        "* Features of input dataset have large differences between their ranges\n",
        "* Zero mean and unit standard deviation are needed\n",
        "* Each of the attributes need to contribute equally to the analysis\n",
        "* Remove outliers impact (standardization does not have a bounding range)"
      ],
      "metadata": {
        "id": "PqqpFlSruQm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization (z score = (x – μ) / σ ) \n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_standardized = scaler.transform(X_train)\n",
        "X_test_standardized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "_N35ohZ0p9-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling - Normalization \n",
        "Normalization is the process of scaling individual samples to have unit norm, and it is useful when:\n",
        "* Distribution of data is unknown.\n",
        "* Quadratic form (dot-product or any other kernel) is used to quantify the similarity of any pair of samples\n",
        "* Features need to be transformed to a similar scale, e.g. [0 1]"
      ],
      "metadata": {
        "id": "TSuTPYBzugA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization ((X - X_min)/(X_max - X_min)) \n",
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer().fit(X_train)\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "5FftlmdYCcv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binarization \n",
        "Binarization is a common operation on text count (presence or absence of a feature), and for estimators that consider boolean random variables (modelled using the Bernoulli distribution in a Bayesian setting)."
      ],
      "metadata": {
        "id": "GH4FMgP_6cat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarization (numerical features to boolean values)\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold = 3).fit(X_train)\n",
        "X_binarized = binarizer.transform(X_test)"
      ],
      "metadata": {
        "id": "OH63JGDLDhtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation\n",
        "It is used to impute the missing values and infer them from the known part of the data.\n",
        "* Univariate imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. impute.SimpleImputer). \n",
        "* Multivariate use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer)."
      ],
      "metadata": {
        "id": "BHGAEAhW7BWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputation of Missing Values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "oFN-iucnFkuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Feature Transforms\n",
        "Input features may interact in many ways, engineering new features can expose these interactions and see potential improved model performance.\n",
        "* Generate a new feature matrix consisting of all specified polynomial combinations of the features degree\n",
        "* Add new interaction features"
      ],
      "metadata": {
        "id": "wINWYH0Xuzxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating polynomial features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(2)\n",
        "X_poly = poly.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "SIyTEK77Mu6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Transformers\n",
        "It is used to convert an existing Python function into a transformer to assist in data cleaning or processing. The the popular log transform returns the natural logarithm of one plus the input array, element-wise.\n",
        "* Log Transform helps to handle skewed data\n",
        "* The distribution becomes more approximate to normal after the transform\n",
        "* Log transform normalizes the magnitude differences\n",
        "* It also decreases the effect of the outliers\n",
        "* Forwards the arguments to a user-defined function"
      ],
      "metadata": {
        "id": "RS-EvfT1u5jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformers\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "transformer = FunctionTransformer(np.log1p, validate=True)\n",
        "X_transformed = transformer.transform(X_train)"
      ],
      "metadata": {
        "id": "7m28sNozNcNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binning \n",
        "Binning is applied to data\n",
        "* Make the model more robust and prevent overfitting\n",
        "* Binning can be applied on both categorical and numerical data\n",
        "* Bin continuous data (features in columns) into intervals\n",
        "* It has a cost to the performance though\n",
        "\n"
      ],
      "metadata": {
        "id": "PxbbGBdTvA9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "k_bins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans').fit(X)\n",
        "est = k_bins.transform(X_train)"
      ],
      "metadata": {
        "id": "NxXHI_QtOMPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Encoding\n",
        "One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. One-hot encoder converts each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector.\n",
        "* Encoding Categorical Features \n",
        "* Use this when attributes are nominal (mutually exclusive)\n",
        "* it can take a multidimensional array"
      ],
      "metadata": {
        "id": "DOXI9YjQvx7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder() \n",
        "enc.fit(y.reshape(-1,1))  \n",
        "enc.transform(y.reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "eiMQYEQlEQl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding\n",
        "Label Encoding converts labels into a numeric form. Machine learning algorithms can then decide in a better way how those labels must be operated.\n",
        "* Use this when attributes are ordinal\n"
      ],
      "metadata": {
        "id": "ghIY5ciWv1O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoder \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "enc = LabelEncoder()\n",
        "y_encoded = enc.fit_transform(y)"
      ],
      "metadata": {
        "id": "7Nxb9a1mJNSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Model Creation"
      ],
      "metadata": {
        "id": "rMBtsU-IOgG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised learning - classfication\n",
        "Supervised learning is an approach to creating AI algorithms to be trained on input data and \"correct\" output. The model is trained until it can detect the underlying patterns and relationships between the input data and the output labels. This enables the model to predict results given new input data.<br> Classification a supervised learning technique and it is used to categorize a set of data into classes."
      ],
      "metadata": {
        "id": "JykTUxfdkPUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Support Vector Machines (SVM)\n",
        "SVMs are based on the idea of finding a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points. The idea is that the data will continue to be mapped into higher and higher dimensions until a hyperplane can be formed to segregate it.<br>\n",
        "<img src=\"https://github.com/mltrends/Machine-Learning-Cheat-Sheets/blob/main/assets/svm3.png?raw=true\" width=\"400\"><br>\n",
        "SVM uses a kernel function to draw Support Vector Classifier in a higher dimension. Types of Kernel Functions are :\n",
        "1. Linear\n",
        "2. Polynomial\n",
        "3. Radial Basis Function(rbf)<br>\n",
        "Kernel function only calculates relationship between every pair of points as if they are in the higher dimensions; they don’t actually do the transformation. This trick , calculating the high dimensional relationships without actually transforming data to the higher dimension, is called the Kernel Trick."
      ],
      "metadata": {
        "id": "P1m4N_dpjRfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Naive Bayes\n",
        "Naive Bayes is based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.<br>\n",
        "$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$<br>"
      ],
      "metadata": {
        "id": "zDbn_MFUoenA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K nearest neighbor (KNN)\n",
        "KNN is a lazy learning algorithm which stores all instances corresponding to training data in n-dimensional space.KNN does not focus on constructing a general internal model, but it works on storing instances of training data.<br>\n",
        "Classification is calculated from a simple majority vote of the k nearest neighbors of each point, so whichever label has most of the neighbors is the label for the new point.<br>\n",
        "<img src=\"https://github.com/mltrends/Machine-Learning-Cheat-Sheets/blob/main/assets/knn1.png?raw=true\" width=\"300\">"
      ],
      "metadata": {
        "id": "GSCvx-15tsYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supervised Learning Estimators - classification\n",
        "# 1. Support Vector Machines (SVM)\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "# 2. Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "# 3. K Nearest Neighbor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knc = KNeighborsClassifier(n_neighbors=3)"
      ],
      "metadata": {
        "id": "B7Ze1meXOqtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Learning - regression\n",
        "Regression is a supervised machine learning technique and it is used to predict continuous values."
      ],
      "metadata": {
        "id": "8dwdlccJBZMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Regression\n",
        "Linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). This method is mostly used for forecasting and finding out cause and effect linear relationship."
      ],
      "metadata": {
        "id": "ihcnFbSt1hgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supervised Learning Estimators - regression\n",
        "# 1. Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "# 2. K Nearest Neighbor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knr = KNeighborsRegressor(n_neighbors=2)"
      ],
      "metadata": {
        "id": "sJ3ZutKnO1n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsupervised learning\n",
        "Unsupervised learning is a type of algorithm which learns patterns from untagged data."
      ],
      "metadata": {
        "id": "YWInYdxXkZ41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K means\n",
        "A K-means clustering algorithm tries to group similar items in the form of clusters. The number of groups is represented by K. \n",
        "1. K points are placed into the object data space representing the initial group of centroids. \n",
        "2. Each data point is then assigned into the closest cluster through reducing the in-cluster variance. \n",
        "3. After all points are assigned, the positions of the k centroids are recalculated using the k clusters. \n",
        "4. Steps 2 and 3 are repeated until the positions of the centroids no longer move.<br>\n",
        "In summary, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the clusters as small as possible.\n",
        "\n",
        "<img src=\"https://github.com/MLtrends/Machine-Learning-Cheat-Sheets/blob/main/assets/k-means.png?raw=true\" width=400>"
      ],
      "metadata": {
        "id": "1J94p53d39Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Principal component analysis (PCA)\n",
        "PCA is a way to bring out strong patterns from large and complex datasets. It finds a way to reduce the dimensions of the data by looking at the eigenvectors of the covariance matrix. The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. <br>\n",
        "\n"
      ],
      "metadata": {
        "id": "FJH-HtYO9xNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsuperviser Learning Estimators\n",
        "# K means\n",
        "from sklearn.cluster import KMeans\n",
        "k_means = KMeans(n_clusters = 3, random_state= 0)\n",
        "# PCA\n",
        "# Reduce number of attributes, while presrving as much info as possible\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_train)"
      ],
      "metadata": {
        "id": "2Wp0iTlXO393"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Fitting"
      ],
      "metadata": {
        "id": "pg5iwSZ6TguD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model fitting\n",
        "# Supervised learning\n",
        "clf = svc.fit(X_train, y_train) # Fit the model to data\n",
        "clf = knc.fit(X_train, y_train) # Fit the model to data\n",
        "clf = gnb.fit(X_train, y_train) # Fit the model to data\n",
        "# Unsupervised learning\n",
        "reg = k_means.fit(X_train)\n",
        "pca_model = pca.fit_transform(X_train)  # Fit to the data, and trannsform it"
      ],
      "metadata": {
        "id": "6Yi8n6WMWPSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prediction"
      ],
      "metadata": {
        "id": "oxtbHiu4TkMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "# Supervised learning\n",
        "y_pred = svc.predict(X_test) # Predict labels\n",
        "y_pred = lr.predict(X_test) # Predict labels\n",
        "y_pred = knc.predict(X_test) # Predict labels\n",
        "# Unsupervised learning\n",
        "y_pred = k_means.predict(X_test) # Predict labels clustering"
      ],
      "metadata": {
        "id": "OA0R2WHmW62p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Performance Evaluation"
      ],
      "metadata": {
        "id": "KrCWNH6RXhnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accuracy score\n",
        "* Accuracy score calculates subset accuracy: it is the set of labels predicted for a sample that matches the corresponding set of labels in y_true.<br>\n",
        "* Classification_report: it is text summary of the precision, recall, F1 score for each class.<br>\n",
        "* Confusion matrix: it is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. It can provide a better idea of what the classification model is getting right and what types of errors it is making."
      ],
      "metadata": {
        "id": "tylMTxJcOvws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifcation Metrics\n",
        "# 1. Accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "knc.score(X_test,y_test)\n",
        "accuracy_score(y_test,y_pred)\n",
        "# 2. Classfication Report\n",
        "from sklearn.metrics import classification_report\n",
        "classification_report(y_test,y_pred)\n",
        "# 3. Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Ff0lSxw3XlDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regression Metrics:\n",
        "* MAE: The Mean Absolute Error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
        "* MSE: Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
        "* RMSE: Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\n",
        "* The coefficient of determination or R-squared represents the proportion of the variance in the dependent variable which is explained by the linear regression model."
      ],
      "metadata": {
        "id": "wLX9fma9s9_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Metrics\n",
        "# 1. Mean Absolute Error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)\n",
        "# 2.Mean Squared Error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_pred)\n",
        "# 3. R^2 score\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "CSv3CpmmYfaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clustering Metrics"
      ],
      "metadata": {
        "id": "NQDRElFk8TZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Rand index: a measure of the percentage of correct decisions made by the algorithm. It can be computed using the following formula: <br>\n",
        "${RI = \\frac{TP+TN}{TP+FP+FN+TN}}$<br>\n",
        "TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives.<br>\n",
        "${ARI = \\frac{RI-expected(RI)}{max(RI)-expected(RI)}}$<br>\n",
        "\n",
        "* Homogeneity score: Clustering results satisfy homogeneity if all its clusters contain only data points that are members of a single class. This metric is independent of the absolute value of labels. It’s defined as:<br>\n",
        "${h = 1- \\frac{H(Y_{true}|Y_{pred})}{H(Y_{true})}}$ <br>\n",
        "\n",
        "* V-measure score: The V-Measure is defined as the harmonic mean of homogeneity h and completeness c of the clustering. Both these measures can be expressed in terms of the mutual information and entropy measures of the information theory.<br>\n",
        "${v = \\frac{(1+beta)*homogeneity*completeness}{beta*homogeneity+completeness}}$"
      ],
      "metadata": {
        "id": "-AE-5IVFFUZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering Metrics\n",
        "# Adjusted Rand Index\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "adjusted_rand_score(y, y_pred)\n",
        "# Homogeneity\n",
        "from sklearn.metrics import homogeneity_score\n",
        "homogeneity_score(y_pred, y_pred)\n",
        "# V-measure\n",
        "from sklearn.metrics import v_measure_score\n",
        "v_measure_score(y, y_pred)"
      ],
      "metadata": {
        "id": "zXFehL6KZdcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross validation\n",
        "Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n"
      ],
      "metadata": {
        "id": "s_gFNPukSVp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "clf = svc.fit(X_train, y_train)\n",
        "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')"
      ],
      "metadata": {
        "id": "jo0FcybIaBl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Tuning"
      ],
      "metadata": {
        "id": "AqGm0-ubafRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* GridSearchCV: It is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. GridSearchCV is used to automate the tuning of hyperparameters to try all possible values (exhaustive method) to know the optimal values.<br>\n",
        "\n",
        "* RandomizedSearchCV: A random search samples a randomly-selected subset of n combinations. The randomized search process requires considerably less compute time and often delivers a similar result. By checking enough randomly-chosen combinations on the grid, the search is likely to identify one that is similar to the one that an exhaustive process would have identified.\n"
      ],
      "metadata": {
        "id": "DWYx_Q9gO73i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(X_train, y_train) \n",
        "# To check the results\n",
        "clf.cv_results_"
      ],
      "metadata": {
        "id": "2UKIpMlzalVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomized Parameter Optimization\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "params ={'n_neighbors': [2,3,4], 'weights':['uniform','distance']}\n",
        "clf = RandomizedSearchCV(estimator=knc,\n",
        "                        param_distributions=params,\n",
        "                        cv=4,\n",
        "                        n_iter=8,\n",
        "                        random_state=5)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.cv_results_"
      ],
      "metadata": {
        "id": "k5XELYXibb8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Pipeline\n",
        "A pipeline can be used to assemble several steps that can be cross-validated together while setting different hyperparameters. It enables setting parameters of the various steps using their names and the parameter name separated by a '__'."
      ],
      "metadata": {
        "id": "tlM_qDEpaFq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.pipeline import Pipeline\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
        "# Making the Pipeline: PCA -> Scaling the data -> Classfication\n",
        "pipe = Pipeline([('pca', PCA()), \n",
        "                 ('scaler', StandardScaler()), \n",
        "                 ('classifer', DecisionTreeClassifier())])\n",
        "# Fitting the model\n",
        "parameters = {'pca__n_components': [2, 3, 4],\n",
        "              'classifer__max_depth': [5, 10, 20]}\n",
        "grid = GridSearchCV(pipe, parameters).fit(X_train, y_train)\n",
        "# Stores the optimum model in best_pipe\n",
        "best_pipe = grid.best_estimator_\n",
        "print(best_pipe)\n",
        "print('Test set score: ' + str(best_pipe.score(X_test,y_test)))"
      ],
      "metadata": {
        "id": "a3vFhADMaKkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f147728-4d72-48c7-f156-f4de5b94d85e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('pca', PCA(n_components=4)), ('scaler', StandardScaler()),\n",
            "                ('classifer', DecisionTreeClassifier(max_depth=5))])\n",
            "Test set score: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}